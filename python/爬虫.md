### 爬虫的流程
* 获取网页
    - 基础技术
        - requests，urllib，selenium
    - 进阶技术
        - 多进程，多线程
        - 登陆抓取
        - 突破IP封禁
        - 使用服务器抓取
* 解析网页，提取数据
    - 基础技术
        - 正则表达式
        - BeautifulSoup
        - lxml
    - 进阶技术
        - 解决中文乱码
* 存储数据
    - 基础技术
        - txt
        - csv
    - 进阶技术
        - mysql
        - redis
        - MongoDB

#### 静态网页抓取
##### requests 库

* 安装  
    pip3 install requests
* get获取网页内容，返回response 对象
    - encoding 返回网页编码
    - status_code 返回响应状态
        - 200 表示成功
        - 4XX表示客户端错误
        - 5xx表示服务器端错误
    - text 返回响应内容
    - content 字节方式返回的响应内容，会自动解析gzip和deflate编码
    - json() 表示requests 内置的json解析器
    - url 表示请求的url
    - get 请求参数设置
        - 使用一个字典来保存需要传递的参数，例如
            dict={'name':'landis','age':35}
            r=requests.get(link,dict)
    - get请求头设置
        - 使用headers 参数进行设置，例如
            headers={'user-agent':'xxxx', 'Host':'www.xxx'}
            r = requests.get(link, headers=headers)
* post 请求
    - 同get 请求，参数以字典的形式传递，例如
    dict={'user_name':'landis','age':18}
    r=requests.post(link,dict)
* 超时
    - 通过方法的 timeout 参数指定，单位是秒，可以设置小数

#### 动态网页抓取
##### 通过浏览器审查元素解析地址
##### 通过selenimu模拟浏览器抓取
###### mac无法打开chromedriver的处理方式
* 下载chrome driver
    - brew install chromedriver
* 设置权限
    sudo xattr -d com.apple.quarantine  chromedriver




